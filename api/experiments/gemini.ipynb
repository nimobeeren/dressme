{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load some images to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garments = {\n",
    "    \"jeans\": {\n",
    "        \"image\": Image.open(\"../../images/garments/bottoms/jeans.webp\"),\n",
    "        \"category\": \"lower_body\",\n",
    "        \"description\": \"jeans\"\n",
    "    },\n",
    "    \"t-shirt\": {\n",
    "        \"image\": Image.open(\"../../images/garments/tops/tshirt.webp\"),\n",
    "        \"category\": \"upper_body\",\n",
    "        \"description\": \"t-shirt\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garments[\"jeans\"][\"image\"].resize((300, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garments[\"t-shirt\"][\"image\"].resize((300, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_image = Image.open(\"../../images/humans/model.jpg\")\n",
    "human_image.resize((300, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the VTON model to put a garment on a human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import replicate\n",
    "import requests\n",
    "\n",
    "\n",
    "def image_to_binary(image: Image.Image) -> io.BytesIO:\n",
    "    stream = io.BytesIO()\n",
    "    image.save(stream, format=\"JPEG\")\n",
    "    stream.seek(0)\n",
    "    return stream\n",
    "\n",
    "\n",
    "def put_garment_on_human(garment, human_image) -> Image.Image:\n",
    "    input = {\n",
    "        \"garm_img\": image_to_binary(garment[\"image\"]),\n",
    "        \"human_img\": image_to_binary(human_image),\n",
    "        \"garment_des\": garment[\"description\"],\n",
    "        \"category\": garment[\"category\"],\n",
    "    }\n",
    "\n",
    "    result_url: str = replicate.run(\n",
    "        \"cuuupid/idm-vton:c871bb9b046607b680449ecbae55fd8c6d945e0a1948644bf2361b3d021d3ff4\",\n",
    "        input=input,\n",
    "    )  # type: ignore\n",
    "\n",
    "    return Image.open(requests.get(result_url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_on_human = put_garment_on_human(garments[\"t-shirt\"], human_image)\n",
    "top_on_human.resize((300, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out where the garment was drawn (image segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig, ThinkingConfig\n",
    "import re\n",
    "from pydantic import BaseModel, TypeAdapter\n",
    "\n",
    "\n",
    "class ObjectDetection(BaseModel):\n",
    "    box_2d: list[int]\n",
    "    mask: str\n",
    "    label: str | None = None\n",
    "\n",
    "\n",
    "def parse_objects(json_text: str) -> list[ObjectDetection]:\n",
    "    text = re.sub(r\"^```json\", \"\", json_text)\n",
    "    text = re.sub(r\"```$\", \"\", text)\n",
    "    objects = TypeAdapter(list[ObjectDetection]).validate_json(text)\n",
    "    return objects\n",
    "\n",
    "\n",
    "def detect_garment(garment, garment_on_human_image: Image.Image) -> ObjectDetection:\n",
    "    # Scale down the garment image\n",
    "    # If both dimensions are <= 384 px it counts as a single tile for Gemini\n",
    "    garment_image = garment[\"image\"].copy()\n",
    "    garment_image.thumbnail((384, 384))\n",
    "    \n",
    "    client = genai.Client()\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=[\n",
    "            garment_on_human_image,\n",
    "            # TODO: adding a second image seems to make the mask output much lower resolution (64x64 vs 256x256 px)\n",
    "            garment_image,\n",
    "            \"\"\"\n",
    "            Give the segmentation mask with respect to the first image, detecting only the garment depicted in the second image.\n",
    "            Output a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \"box_2d\", the segmentation mask in key \"mask\", and the text label in the key \"label\". Use descriptive labels.\n",
    "            \"\"\".strip(),\n",
    "        ],\n",
    "        config=GenerateContentConfig(\n",
    "            # Google says it works better without thinking\n",
    "            thinking_config=ThinkingConfig(thinking_budget=0),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    assert response.text is not None\n",
    "    print(\"Raw response:\", response.text)\n",
    "\n",
    "    assert response.usage_metadata is not None\n",
    "    print(\"Input tokens:\", response.usage_metadata.prompt_token_count)\n",
    "    print(\"Thinking tokens:\", response.usage_metadata.thoughts_token_count)\n",
    "    print(\"Output tokens:\", response.usage_metadata.candidates_token_count)\n",
    "\n",
    "    objects = parse_objects(response.text)\n",
    "    return objects[0]  # assume there is only one object: our garment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = detect_garment(garments[\"t-shirt\"], top_on_human)\n",
    "detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the image segmentation result as a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def get_mask_image(detection: ObjectDetection):\n",
    "    # Remove the data URL prefix if present\n",
    "    mask_b64 = re.sub(r\"^data:image/png;base64,\", \"\", detection.mask)\n",
    "    mask_bytes = base64.b64decode(mask_b64)\n",
    "    print(len(mask_bytes), \"bytes\")\n",
    "    image = Image.open(BytesIO(mask_bytes)).convert(\"L\")  # Grayscale\n",
    "    print(f\"{image.size[0]}x{image.size[1]} px\")\n",
    "    return image\n",
    "\n",
    "\n",
    "mask_image = get_mask_image(detection)\n",
    "mask_image.resize((200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import Resampling\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def resize_mask(\n",
    "    mask_image: Image.Image, garment_on_human_image: Image.Image\n",
    ") -> Image.Image:\n",
    "    # Our data is [ymin, xmin, ymax, xmax] in 0-1000\n",
    "    ymin, xmin, ymax, xmax = detection.box_2d\n",
    "    w, h = garment_on_human_image.size\n",
    "\n",
    "    # Normalize if any value > 1000 (as in JS)\n",
    "    if any(v > 1000 for v in (ymin, xmin, ymax, xmax)):\n",
    "        ymin = (ymin / h) * 1000\n",
    "        xmin = (xmin / w) * 1000\n",
    "        ymax = (ymax / h) * 1000\n",
    "        xmax = (xmax / w) * 1000\n",
    "\n",
    "    # Convert to normalized [0,1] coordinates\n",
    "    nYmin, nXmin, nYmax, nXmax = [v / 1000 for v in (ymin, xmin, ymax, xmax)]\n",
    "\n",
    "    # Compute pixel coordinates\n",
    "    left = int(round(nXmin * w))\n",
    "    top = int(round(nYmin * h))\n",
    "    right = int(round(nXmax * w))\n",
    "    bottom = int(round(nYmax * h))\n",
    "\n",
    "    box_width = max(1, right - left)\n",
    "    box_height = max(1, bottom - top)\n",
    "\n",
    "    # Resize the mask to fit the bounding box\n",
    "    mask_box_resized = mask_image.resize(\n",
    "        (box_width, box_height), resample=Resampling.BILINEAR\n",
    "    )\n",
    "\n",
    "    # Paste the resized mask into a full-size mask at the correct position\n",
    "    mask_resized = Image.new(\"L\", garment_on_human_image.size, 0)\n",
    "    mask_resized.paste(mask_box_resized, (left, top))\n",
    "\n",
    "    return mask_resized\n",
    "\n",
    "\n",
    "mask_resized = resize_mask(mask_image, top_on_human)\n",
    "mask_resized.resize((300, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_mask(\n",
    "    mask_resized: Image.Image, garment_on_human_image: Image.Image\n",
    ") -> Image.Image:\n",
    "    # Create a color overlay (e.g., red) with 50% opacity where mask is present\n",
    "    overlay = Image.new(\"RGBA\", garment_on_human_image.size, (255, 0, 0, 0))\n",
    "    mask_np = np.array(mask_resized)\n",
    "    alpha = (mask_np > 0) * 128  # 50% opacity where mask is present\n",
    "    overlay_np = np.array(overlay)\n",
    "    overlay_np[..., 3] = alpha\n",
    "    overlay = Image.fromarray(overlay_np, mode=\"RGBA\")\n",
    "\n",
    "    # Composite overlay onto garment_on_human\n",
    "    rgba = garment_on_human_image.convert(\"RGBA\")\n",
    "    return Image.alpha_composite(rgba, overlay)\n",
    "\n",
    "\n",
    "overlay = overlay_mask(mask_resized, top_on_human)\n",
    "overlay.resize((540, 720))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do the same with a bottom garment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_on_human = put_garment_on_human(garments[\"jeans\"], human_image)\n",
    "bottom_on_human.resize((300, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And combine the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_garments(\n",
    "    human_image: Image.Image,\n",
    "    top_im: Image.Image,\n",
    "    bottom_im: Image.Image,\n",
    "    top_mask_im: Image.Image,\n",
    "):\n",
    "    result = Image.new(\"RGB\", human_image.size)\n",
    "    result.paste(bottom_im, (0, 0))\n",
    "    result.paste(top_im, (0, 0), top_mask_im.convert(\"L\"))\n",
    "    return result\n",
    "\n",
    "\n",
    "final_image = combine_garments(human_image, top_on_human, bottom_on_human, mask_resized)\n",
    "final_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
